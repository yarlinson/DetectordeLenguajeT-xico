# FLUJO DE PROCESAMIENTO DETALLADO - DETECTOR DE LENGUAJE T√ìXICO

## üìã RESUMEN DEL PIPELINE

El sistema procesa texto en **6 pasos principales**:
1. **Normalizaci√≥n del texto**
2. **B√∫squeda de patrones con regex**
3. **Registro de patrones encontrados**
4. **Clasificaci√≥n con AFD (transiciones)**
5. **Determinaci√≥n del nivel final**
6. **C√°lculo de confianza**

---

## üîç PASO 1: NORMALIZACI√ìN DEL TEXTO

**¬øQu√© hace?** Convierte el texto a min√∫sculas para b√∫squeda insensible a may√∫sculas.

**Ubicaci√≥n en el c√≥digo:**
```python
# Archivo: detector/automaton.py
# L√≠nea: 157

def process_text(self, text: str) -> Dict:
    self.reset()
    text_lower = text.lower()  # ‚Üê AQU√ç: Normalizaci√≥n
```

**C√≥digo completo:**
```python
# detector/automaton.py, l√≠nea 141-157
def process_text(self, text: str) -> Dict:
    """
    Procesa el texto de entrada usando expresiones regulares para encontrar patrones
    y un AFD para clasificar los resultados.
    """
    # Inicializar el AFD en el estado inicial q0
    self.reset()  # L√≠nea 156
    text_lower = text.lower()  # ‚Üê PASO 1: Normalizaci√≥n (L√≠nea 157)
```

**Tambi√©n se llama desde:**
- `detector/views.py`, l√≠nea 30: `analysis_result = toxic_detector.process_text(text)`
- `detector/views.py`, l√≠nea 88: `analysis_result = toxic_detector.process_text(text)`

---

## üîç PASO 2: B√öSQUEDA DE PATRONES CON EXPRESIONES REGULARES

**¬øQu√© hace?** Itera sobre todos los patrones regex y busca coincidencias en el texto.

**Ubicaci√≥n en el c√≥digo:**
```python
# Archivo: detector/automaton.py
# L√≠neas: 179-183

# Buscar patrones usando expresiones regulares
for toxicity_type, patterns in self.toxic_patterns.items():  # L√≠nea 180
    for pattern in patterns:  # L√≠nea 181
        # Buscar coincidencias usando regex
        if re.search(pattern, text_lower, re.IGNORECASE):  # ‚Üê PASO 2: B√∫squeda regex (L√≠nea 183)
```

**C√≥digo completo:**
```python
# detector/automaton.py, l√≠neas 179-187
# PASO 1: USAR EXPRESIONES REGULARES PARA ENCONTRAR PATRONES
# Mapeo de tipos de toxicidad a estados del AFD
type_to_state = {
    ToxicityType.INSULT: 'q1',
    ToxicityType.THREAT: 'q2',
    ToxicityType.HATE: 'q3',
    ToxicityType.HARASSMENT: 'q4',
    ToxicityType.PROFANITY: 'q5'
}

# Buscar patrones usando expresiones regulares
for toxicity_type, patterns in self.toxic_patterns.items():  # L√≠nea 180
    for pattern in patterns:  # L√≠nea 181
        # Buscar coincidencias usando regex
        if re.search(pattern, text_lower, re.IGNORECASE):  # ‚Üê PASO 2 (L√≠nea 183)
            # Guardar informaci√≥n del patr√≥n encontrado
            detected_toxicity['is_toxic'] = True  # L√≠nea 185
            detected_toxicity['types'].append(toxicity_type.value)  # L√≠nea 186
            detected_toxicity['matched_patterns'].append(pattern)  # L√≠nea 187
```

**Patrones definidos en:**
- `detector/automaton.py`, l√≠neas 91-119: M√©todo `_load_toxic_patterns()`

**Ejemplo de patr√≥n:**
```python
# detector/automaton.py, l√≠neas 94-97
ToxicityType.INSULT: [
    r'\b(est√∫pido|idiota|imb√©cil|tonto|burro|animal|cerdo|basura|mierda)\b',
    r'\b(puto|puta|hijo de puta|cabr√≥n|gilipollas|capullo)\b',
    r'\b(in√∫til|fracasado|perdedor|basura humana)\b'
]
```

---

## üìù PASO 3: REGISTRO DE PATRONES ENCONTRADOS

**¬øQu√© hace?** Guarda informaci√≥n sobre cada patr√≥n que coincide: tipo, patr√≥n regex, y marca el texto como t√≥xico.

**Ubicaci√≥n en el c√≥digo:**
```python
# Archivo: detector/automaton.py
# L√≠neas: 184-187

if re.search(pattern, text_lower, re.IGNORECASE):
    # Guardar informaci√≥n del patr√≥n encontrado
    detected_toxicity['is_toxic'] = True  # ‚Üê PASO 3: Marcar como t√≥xico (L√≠nea 185)
    detected_toxicity['types'].append(toxicity_type.value)  # ‚Üê PASO 3: Guardar tipo (L√≠nea 186)
    detected_toxicity['matched_patterns'].append(pattern)  # ‚Üê PASO 3: Guardar patr√≥n (L√≠nea 187)
```

**C√≥digo completo:**
```python
# detector/automaton.py, l√≠neas 183-187
if re.search(pattern, text_lower, re.IGNORECASE):
    # PASO 3: REGISTRO DE PATRONES ENCONTRADOS
    detected_toxicity['is_toxic'] = True  # L√≠nea 185
    detected_toxicity['types'].append(toxicity_type.value)  # L√≠nea 186
    detected_toxicity['matched_patterns'].append(pattern)  # L√≠nea 187
```

**Estructura de `detected_toxicity`:**
```python
# detector/automaton.py, l√≠neas 159-167
detected_toxicity = {
    'is_toxic': False,  # Se actualiza en PASO 3
    'level': ToxicityLevel.SAFE,
    'types': [],  # Se actualiza en PASO 3
    'matched_patterns': [],  # Se actualiza en PASO 3
    'confidence': 0.0,
    'original_text': text,
    'state_path': [self.current_state.name]  # Se actualiza en PASO 4
}
```

---

## üîÑ PASO 4: CLASIFICACI√ìN CON AFD (TRANSICIONES)

**¬øQu√© hace?** Realiza transiciones en el AFD bas√°ndose en los patrones encontrados. Si encuentra un patr√≥n, transiciona de q‚ÇÄ a q‚ÇÅ-q‚ÇÖ. Si ya est√° en un estado t√≥xico y encuentra otro patr√≥n, transiciona a q‚ÇÜ.

**Ubicaci√≥n en el c√≥digo:**
```python
# Archivo: detector/automaton.py
# L√≠neas: 189-194

# PASO 2: USAR EL AFD PARA CLASIFICAR
# Realizar transici√≥n en el AFD basada en el tipo de toxicidad encontrado
target_state = type_to_state.get(toxicity_type)  # L√≠nea 191
if target_state:
    self._transition_to_state(target_state)  # ‚Üê PASO 4: Transici√≥n AFD (L√≠nea 193)
    detected_toxicity['state_path'].append(self.current_state.name)  # L√≠nea 194
```

**Funci√≥n de transici√≥n:**
```python
# detector/automaton.py, l√≠neas 125-139
def _transition_to_state(self, target_state_name: str):
    """
    Realiza una transici√≥n del AFD a un estado espec√≠fico.
    """
    if target_state_name in self.states:
        # Si ya estamos en un estado t√≥xico (q1-q5) y encontramos otro patr√≥n,
        # transicionar al estado final q6
        if self.current_state.name in ['q1', 'q2', 'q3', 'q4', 'q5']:  # L√≠nea 135
            self.current_state = self.states['q6']  # ‚Üê Transici√≥n a q6 (L√≠nea 136)
        else:
            # Transici√≥n desde q0 (estado inicial) al estado de detecci√≥n
            self.current_state = self.states[target_state_name]  # ‚Üê Transici√≥n q0‚Üíq1/q2/q3/q4/q5 (L√≠nea 139)
```

**Mapeo de tipos a estados:**
```python
# detector/automaton.py, l√≠neas 171-177
type_to_state = {
    ToxicityType.INSULT: 'q1',      # Insulto ‚Üí q1
    ToxicityType.THREAT: 'q2',      # Amenaza ‚Üí q2
    ToxicityType.HATE: 'q3',         # Odio ‚Üí q3
    ToxicityType.HARASSMENT: 'q4',   # Acoso ‚Üí q4
    ToxicityType.PROFANITY: 'q5'     # Profanidad ‚Üí q5
}
```

**Inicializaci√≥n del AFD:**
```python
# detector/automaton.py, l√≠neas 121-123
def reset(self):
    """Reinicia el aut√≥mata al estado inicial."""
    self.current_state = self.initial_state  # ‚Üê Vuelve a q0
```

**Estados definidos:**
```python
# detector/automaton.py, l√≠neas 78-86
self.states = {
    'q0': State('q0', False, ToxicityLevel.SAFE),  # Estado inicial
    'q1': State('q1', False, ToxicityLevel.LOW, ToxicityType.INSULT),
    'q2': State('q2', False, ToxicityLevel.MEDIUM, ToxicityType.THREAT),
    'q3': State('q3', False, ToxicityLevel.HIGH, ToxicityType.HATE),
    'q4': State('q4', False, ToxicityLevel.MEDIUM, ToxicityType.HARASSMENT),
    'q5': State('q5', False, ToxicityLevel.LOW, ToxicityType.PROFANITY),
    'q6': State('q6', True, ToxicityLevel.EXTREME)  # Estado final t√≥xico
}
```

---

## üéØ PASO 5: DETERMINACI√ìN DEL NIVEL FINAL DE TOXICIDAD

**¬øQu√© hace?** Determina el nivel final de toxicidad bas√°ndose en el estado final del AFD.

**Ubicaci√≥n en el c√≥digo:**
```python
# Archivo: detector/automaton.py
# L√≠neas: 196-208

# Clasificaci√≥n final basada en el estado del AFD
# El estado final del AFD determina el nivel de toxicidad
final_state = self.current_state  # ‚Üê PASO 5: Obtener estado final (L√≠nea 198)

if final_state.name != 'q0':  # Si no estamos en el estado seguro
    detected_toxicity['level'] = final_state.toxicity_level  # ‚Üê PASO 5: Asignar nivel (L√≠nea 201)
    
    # Si llegamos al estado final q6, el nivel es extremo
    if final_state.name == 'q6':  # L√≠nea 204
        detected_toxicity['level'] = ToxicityLevel.EXTREME  # ‚Üê PASO 5: Nivel extremo (L√≠nea 205)
else:
    # Si no se encontraron patrones, permanecemos en q0 (seguro)
    detected_toxicity['level'] = ToxicityLevel.SAFE  # ‚Üê PASO 5: Nivel seguro (L√≠nea 208)
```

**C√≥digo completo:**
```python
# detector/automaton.py, l√≠neas 196-208
# Clasificaci√≥n final basada en el estado del AFD
# El estado final del AFD determina el nivel de toxicidad
final_state = self.current_state  # L√≠nea 198

if final_state.name != 'q0':  # Si no estamos en el estado seguro
    detected_toxicity['level'] = final_state.toxicity_level  # L√≠nea 201
    
    # Si llegamos al estado final q6, el nivel es extremo
    if final_state.name == 'q6':  # L√≠nea 204
        detected_toxicity['level'] = ToxicityLevel.EXTREME  # L√≠nea 205
else:
    # Si no se encontraron patrones, permanecemos en q0 (seguro)
    detected_toxicity['level'] = ToxicityLevel.SAFE  # L√≠nea 208
```

**Mapeo de estados a niveles:**
- **q‚ÇÄ** ‚Üí `ToxicityLevel.SAFE` (Seguro)
- **q‚ÇÅ** ‚Üí `ToxicityLevel.LOW` (Bajo - Insultos)
- **q‚ÇÇ** ‚Üí `ToxicityLevel.MEDIUM` (Medio - Amenazas)
- **q‚ÇÉ** ‚Üí `ToxicityLevel.HIGH` (Alto - Odio)
- **q‚ÇÑ** ‚Üí `ToxicityLevel.MEDIUM` (Medio - Acoso)
- **q‚ÇÖ** ‚Üí `ToxicityLevel.LOW` (Bajo - Profanidad)
- **q‚ÇÜ** ‚Üí `ToxicityLevel.EXTREME` (Extremo - M√∫ltiples tipos)

---

## üìä PASO 6: C√ÅLCULO DE CONFIANZA

**¬øQu√© hace?** Calcula el nivel de confianza del an√°lisis bas√°ndose en el n√∫mero de patrones encontrados.

**Ubicaci√≥n en el c√≥digo:**
```python
# Archivo: detector/automaton.py
# L√≠neas: 210-215

# Eliminar tipos duplicados
detected_toxicity['types'] = list(set(detected_toxicity['types']))  # L√≠nea 211

# Calcular confianza basada en el n√∫mero de patrones encontrados
if detected_toxicity['is_toxic']:  # L√≠nea 214
    detected_toxicity['confidence'] = min(1.0, len(detected_toxicity['matched_patterns']) * 0.2)  # ‚Üê PASO 6 (L√≠nea 215)
```

**F√≥rmula de confianza:**
```
confianza = min(1.0, n√∫mero_de_patrones √ó 0.2)
```

**Ejemplos:**
- 1 patr√≥n ‚Üí confianza = 0.2 (20%)
- 2 patrones ‚Üí confianza = 0.4 (40%)
- 3 patrones ‚Üí confianza = 0.6 (60%)
- 4 patrones ‚Üí confianza = 0.8 (80%)
- 5+ patrones ‚Üí confianza = 1.0 (100%)

---

## üîÑ FLUJO COMPLETO EN EL SISTEMA

### **1. Entrada del usuario (Vista)**
```python
# detector/views.py, l√≠neas 24-30
if request.method == 'POST':
    form = TextAnalysisForm(request.POST)
    if form.is_valid():
        text = form.cleaned_data['text']  # Texto del usuario
        
        # Realizar an√°lisis con el aut√≥mata
        analysis_result = toxic_detector.process_text(text)  # ‚Üê Llama al pipeline
```

### **2. Procesamiento (Aut√≥mata)**
```python
# detector/automaton.py, l√≠nea 141
def process_text(self, text: str) -> Dict:
    # PASO 1: Normalizaci√≥n (l√≠nea 157)
    # PASO 2: B√∫squeda regex (l√≠neas 180-183)
    # PASO 3: Registro (l√≠neas 185-187)
    # PASO 4: Transiciones AFD (l√≠neas 191-194)
    # PASO 5: Nivel final (l√≠neas 196-208)
    # PASO 6: Confianza (l√≠neas 210-215)
    return detected_toxicity
```

### **3. Guardado en base de datos (Vista)**
```python
# detector/views.py, l√≠neas 33-43
analysis = TextAnalysis.objects.create(
    text=text,
    is_toxic=analysis_result['is_toxic'],
    toxicity_level=analysis_result['level'].value,
    toxicity_types=analysis_result['types'],
    matched_patterns=analysis_result['matched_patterns'],
    confidence=analysis_result['confidence'],
    user=request.user if request.user.is_authenticated else None,
    ip_address=get_client_ip(request),
    user_agent=request.META.get('HTTP_USER_AGENT', '')
)
```

### **4. Actualizaci√≥n de estad√≠sticas (Vista)**
```python
# detector/views.py, l√≠nea 46
update_statistics(analysis)  # Actualiza estad√≠sticas diarias
```

### **5. Visualizaci√≥n (Template)**
```python
# detector/views.py, l√≠neas 50-53
return render(request, 'detector/result.html', {
    'analysis': analysis,
    'analysis_result': analysis_result
})
```

---

## üìç RESUMEN DE UBICACIONES EN EL C√ìDIGO

| Paso | Descripci√≥n | Archivo | L√≠neas |
|------|-------------|---------|-------|
| **1** | Normalizaci√≥n del texto | `detector/automaton.py` | 157 |
| **2** | B√∫squeda con regex | `detector/automaton.py` | 180-183 |
| **3** | Registro de patrones | `detector/automaton.py` | 185-187 |
| **4** | Transiciones AFD | `detector/automaton.py` | 125-139, 191-194 |
| **5** | Nivel final | `detector/automaton.py` | 196-208 |
| **6** | C√°lculo de confianza | `detector/automaton.py` | 210-215 |
| **Inicializaci√≥n** | Reset del AFD | `detector/automaton.py` | 121-123, 156 |
| **Llamada desde vista** | Invocaci√≥n del pipeline | `detector/views.py` | 30, 88 |
| **Guardado en BD** | Persistencia de resultados | `detector/views.py` | 33-43 |
| **Estad√≠sticas** | Actualizaci√≥n de m√©tricas | `detector/views.py` | 46, 262-317 |

---

## üéØ EJEMPLO COMPLETO DE EJECUCI√ìN

**Texto de entrada:** `"Eres un est√∫pido y te voy a matar"`

### **Paso 1: Normalizaci√≥n**
```python
text_lower = "eres un est√∫pido y te voy a matar"
# L√≠nea 157: text.lower()
```

### **Paso 2: B√∫squeda regex**
```python
# L√≠nea 183: re.search(pattern, text_lower, re.IGNORECASE)
# Encuentra: "est√∫pido" ‚Üí ToxicityType.INSULT
# Encuentra: "te voy a matar" ‚Üí ToxicityType.THREAT
```

### **Paso 3: Registro**
```python
# L√≠neas 185-187
detected_toxicity['is_toxic'] = True
detected_toxicity['types'] = ['insult', 'threat']
detected_toxicity['matched_patterns'] = [
    r'\b(est√∫pido|idiota|...)\b',
    r'\b(te voy a matar|te mato|...)\b'
]
```

### **Paso 4: Transiciones AFD**
```python
# Primera transici√≥n (l√≠nea 193)
# q‚ÇÄ ‚Üí q‚ÇÅ (por insulto)
# state_path = ['q0', 'q1']

# Segunda transici√≥n (l√≠nea 193)
# q‚ÇÅ ‚Üí q‚ÇÜ (por amenaza adicional)
# state_path = ['q0', 'q1', 'q6']
```

### **Paso 5: Nivel final**
```python
# L√≠neas 198-205
final_state = q‚ÇÜ
detected_toxicity['level'] = ToxicityLevel.EXTREME
```

### **Paso 6: Confianza**
```python
# L√≠nea 215
detected_toxicity['confidence'] = min(1.0, 2 * 0.2) = 0.4 (40%)
```

### **Resultado final:**
```python
{
    'is_toxic': True,
    'level': ToxicityLevel.EXTREME,
    'types': ['insult', 'threat'],
    'matched_patterns': [...],
    'confidence': 0.4,
    'state_path': ['q0', 'q1', 'q6']
}
```

---

**Fin del documento**

