# ExplicaciÃ³n del Recorrido del AFD (AutÃ³mata Finito Determinista)

## ğŸ“‹ Componentes del AFD

**QuÃ­ntupla del AFD: M = (Q, Î£, Î´, qâ‚€, F)**

- **Q (Estados)**: `{qâ‚€, qâ‚, qâ‚‚, qâ‚ƒ}`
  - `qâ‚€`: Estado inicial â†’ **SAFE** (Seguro)
  - `qâ‚`: Estado â†’ **LOW** (Bajo)
  - `qâ‚‚`: Estado â†’ **MEDIUM** (Medio)
  - `qâ‚ƒ`: Estado â†’ **EXTREME** (Extremo) - **Estado absorbente**

- **Î£ (Alfabeto)**: Patrones regex de toxicidad organizados por tipo:
  - `INSULT` (Insultos)
  - `PROFANITY` (Profanidad)
  - `HARASSMENT` (Acoso)
  - `THREAT` (Amenazas)
  - `HATE` (Odio)

- **Î´ (FunciÃ³n de TransiciÃ³n)**: Reglas que definen cÃ³mo cambiar de estado segÃºn el patrÃ³n detectado

- **qâ‚€**: Estado inicial (`q0`)

- **F (Estados Finales)**: Todos los estados son finales `{qâ‚€, qâ‚, qâ‚‚, qâ‚ƒ}`

---

## ğŸ”„ Tabla de Transiciones

| Estado Actual | PatrÃ³n Detectado | Nuevo Estado | Nivel de Toxicidad |
|---------------|------------------|--------------|-------------------|
| qâ‚€ | INSULT, PROFANITY | qâ‚ | LOW |
| qâ‚€ | HARASSMENT | qâ‚‚ | MEDIUM |
| qâ‚€ | THREAT, HATE | qâ‚ƒ | EXTREME |
| qâ‚ | INSULT, PROFANITY | qâ‚ | LOW (permanece) |
| qâ‚ | HARASSMENT | qâ‚‚ | MEDIUM |
| qâ‚ | THREAT, HATE | qâ‚ƒ | EXTREME |
| qâ‚‚ | INSULT, PROFANITY, HARASSMENT | qâ‚‚ | MEDIUM (permanece) |
| qâ‚‚ | THREAT, HATE | qâ‚ƒ | EXTREME |
| qâ‚ƒ | **Cualquier patrÃ³n** | qâ‚ƒ | EXTREME (absorbente) |

---

## ğŸ“ Ejemplo Paso a Paso

### Ejemplo 1: Texto Simple
**Texto de entrada**: `"Eres un idiota"`

#### Paso 1: InicializaciÃ³n
```python
# En automaton.py, lÃ­nea 190
self.reset()  # current_state = 'q0'
detected_toxicity = {
    'is_toxic': False,
    'level': ToxicityLevel.SAFE,
    'state': 'q0',
    'types': [],
    'matched_patterns': [],
    'detected_words': [],
    'confidence': 0.0
}
```

**Estado del AFD**: `qâ‚€` (SAFE)

#### Paso 2: BÃºsqueda de Patrones
```python
# LÃ­neas 207-211: Se itera sobre todos los patrones
for toxicity_type, patterns in self.toxic_patterns.items():
    for pattern in patterns:
        for match in re.finditer(pattern, text, re.IGNORECASE):
            # Busca "idiota" en el texto
```

**PatrÃ³n encontrado**: `"idiota"` â†’ Tipo: `INSULT`

#### Paso 3: Aplicar TransiciÃ³n
```python
# LÃ­nea 234: Aplicar funciÃ³n de transiciÃ³n
new_state = self._transition(self.current_state, toxicity_type)
# _transition('q0', INSULT) â†’ retorna 'q1'
```

**FunciÃ³n de transiciÃ³n** (lÃ­neas 143-145):
```python
if current_state == 'q0':
    if toxicity_type in [ToxicityType.INSULT, ToxicityType.PROFANITY]:
        return 'q1'  # LOW
```

**Estado del AFD**: `qâ‚€` â†’ `qâ‚` (LOW)

#### Paso 4: Actualizar InformaciÃ³n
```python
# LÃ­neas 238-242
detected_toxicity['is_toxic'] = True
detected_toxicity['types'].append('insult')
detected_toxicity['matched_patterns'].append(pattern)
detected_toxicity['detected_words'].append(DetectedWord(...))
```

#### Paso 5: Calcular Nivel Final
```python
# LÃ­neas 244-246
detected_toxicity['level'] = self._get_level_from_state('q1')
# Retorna: ToxicityLevel.LOW
```

#### Paso 6: Calcular Confianza
```python
# LÃ­neas 249-251
num_words = 1  # Se detectÃ³ 1 palabra
confidence = min(1.0, 1 * 0.15) = 0.15  # 15%
```

**Resultado Final**:
```python
{
    'is_toxic': True,
    'level': ToxicityLevel.LOW,
    'state': 'q1',
    'types': ['insult'],
    'confidence': 0.15,
    'detected_words': [DetectedWord(text='idiota', start=9, end=15, ...)]
}
```

---

### Ejemplo 2: Texto con MÃºltiples Patrones
**Texto de entrada**: `"Eres un idiota, ojalÃ¡ te mueras"`

#### Recorrido del AFD:

1. **Estado inicial**: `qâ‚€` (SAFE)

2. **Detecta "idiota"** (INSULT):
   - `Î´(qâ‚€, INSULT) = qâ‚`
   - **Estado actual**: `qâ‚` (LOW)

3. **Detecta "ojalÃ¡ te mueras"** (THREAT):
   - `Î´(qâ‚, THREAT) = qâ‚ƒ`
   - **Estado actual**: `qâ‚ƒ` (EXTREME)

4. **Resultado**: 
   - Nivel: **EXTREME** (qâ‚ƒ)
   - Tipos: `['insult', 'threat']`
   - Confianza: `min(1.0, 2 * 0.15) = 0.30` (30%)

---

### Ejemplo 3: Estado Absorbente
**Texto de entrada**: `"Te voy a matar, eres basura, muÃ©rete"`

#### Recorrido:

1. **qâ‚€** â†’ Detecta "matar" (THREAT) â†’ **qâ‚ƒ** (EXTREME)

2. **qâ‚ƒ** â†’ Detecta "basura" (INSULT) â†’ **qâ‚ƒ** (permanece en EXTREME)
   ```python
   # LÃ­nea 139: Estado absorbente
   if current_state == 'q3':
       return 'q3'  # Siempre permanece en q3
   ```

3. **qâ‚ƒ** â†’ Detecta "muÃ©rete" (THREAT) â†’ **qâ‚ƒ** (permanece)

**Resultado**: Nivel **EXTREME** (una vez que llega a qâ‚ƒ, nunca baja)

---

## ğŸ” Flujo Completo en el CÃ³digo

```
1. process_text(text) [lÃ­nea 180]
   â”‚
   â”œâ”€> reset() [lÃ­nea 190]
   â”‚   â””â”€> current_state = 'q0'
   â”‚
   â”œâ”€> Bucle: Buscar patrones [lÃ­neas 207-242]
   â”‚   â”‚
   â”‚   â”œâ”€> re.finditer(pattern, text) [lÃ­nea 211]
   â”‚   â”‚   â””â”€> Encuentra coincidencia
   â”‚   â”‚
   â”‚   â”œâ”€> _transition(current_state, toxicity_type) [lÃ­nea 234]
   â”‚   â”‚   â””â”€> Calcula nuevo estado segÃºn tabla de transiciones
   â”‚   â”‚
   â”‚   â”œâ”€> Actualiza current_state [lÃ­nea 235]
   â”‚   â”‚
   â”‚   â””â”€> Guarda palabra detectada [lÃ­neas 224-231]
   â”‚
   â”œâ”€> _get_level_from_state(current_state) [lÃ­nea 245]
   â”‚   â””â”€> Convierte estado (q0-q3) a nivel (SAFE-EXTREME)
   â”‚
   â”œâ”€> Calcula confianza [lÃ­neas 249-251]
   â”‚
   â””â”€> _highlight_detected_words() [lÃ­nea 254]
       â””â”€> Genera HTML con palabras resaltadas
```

---

## ğŸ’¡ Puntos Clave

1. **El AFD procesa el texto de izquierda a derecha**, aplicando transiciones cada vez que encuentra un patrÃ³n.

2. **El estado final es el mÃ¡s alto alcanzado**: Si el texto pasa por qâ‚€ â†’ qâ‚ â†’ qâ‚ƒ, el resultado es qâ‚ƒ (EXTREME).

3. **qâ‚ƒ es absorbente**: Una vez que el AFD llega a qâ‚ƒ, permanece ahÃ­ sin importar quÃ© mÃ¡s detecte.

4. **MÃºltiples patrones del mismo tipo**: Si detecta varios insultos, permanece en qâ‚ (LOW) hasta que detecte algo mÃ¡s grave.

5. **La confianza se calcula al final**: Basada en el nÃºmero total de palabras detectadas (mÃ¡ximo 1.0 = 100%).

---

## ğŸ¯ Resumen Visual

```
Texto: "Eres un idiota, ojalÃ¡ te mueras"
       â†“
[Inicio: qâ‚€ (SAFE)]
       â†“
Detecta "idiota" (INSULT)
       â†“
[qâ‚ (LOW)]
       â†“
Detecta "ojalÃ¡ te mueras" (THREAT)
       â†“
[qâ‚ƒ (EXTREME)] â† Estado final
       â†“
Resultado: EXTREME, tipos: [insult, threat], confianza: 30%
```

