# Recorrido del AFD en el CÃ³digo - ExplicaciÃ³n Detallada

## ğŸ¯ Ejemplo PequeÃ±o: "Eres un idiota"

### Paso 1: InicializaciÃ³n (lÃ­nea 190 en `automaton.py`)

```python
def process_text(self, text: str) -> Dict:
    self.reset()  # current_state = 'q0'
```

**Estado del AFD**: `q0` (SAFE)

---

### Paso 2: BÃºsqueda de Patrones (lÃ­neas 207-211)

```python
for toxicity_type, patterns in self.toxic_patterns.items():
    for pattern in patterns:
        for match in re.finditer(pattern, text, re.IGNORECASE):
```

**AcciÃ³n**: Busca el patrÃ³n `"idiota"` en el texto `"Eres un idiota"`

**Resultado**: âœ… Encuentra coincidencia en la posiciÃ³n 9-15

---

### Paso 3: Crear Objeto DetectedWord (lÃ­neas 224-231)

```python
detected_word = DetectedWord(
    text="idiota",
    start=9,
    end=15,
    pattern=r"\bidiota\b",
    toxicity_type=ToxicityType.INSULT
)
detected_toxicity['detected_words'].append(detected_word)
```

**InformaciÃ³n guardada**:
- Texto detectado: `"idiota"`
- PosiciÃ³n: caracteres 9-15
- Tipo: `INSULT`

---

### Paso 4: Aplicar TransiciÃ³n del AFD (lÃ­neas 234-235)

```python
new_state = self._transition(self.current_state, toxicity_type)
self.current_state = new_state
```

**Llamada a `_transition`** (lÃ­neas 127-168):

```python
def _transition(self, current_state: str, toxicity_type: ToxicityType) -> str:
    if current_state == 'q0':  # â† Estamos aquÃ­
        if toxicity_type in [ToxicityType.INSULT, ToxicityType.PROFANITY]:
            return 'q1'  # â† Retorna q1
```

**TransiciÃ³n aplicada**: 
- Estado anterior: `q0` (SAFE)
- PatrÃ³n detectado: `INSULT`
- FunciÃ³n: `Î´(q0, INSULT) = q1`
- Estado nuevo: `q1` (LOW)

---

### Paso 5: Actualizar InformaciÃ³n (lÃ­neas 238-242)

```python
detected_toxicity['is_toxic'] = True
if toxicity_type.value not in detected_toxicity['types']:
    detected_toxicity['types'].append(toxicity_type.value)  # AÃ±ade 'insult'
if pattern not in detected_toxicity['matched_patterns']:
    detected_toxicity['matched_patterns'].append(pattern)
```

**Estado actualizado**:
- `is_toxic`: `True`
- `types`: `['insult']`
- `matched_patterns`: `[r'\bidiota\b']`

---

### Paso 6: Calcular Nivel Final (lÃ­neas 244-246)

```python
detected_toxicity['level'] = self._get_level_from_state(self.current_state)
detected_toxicity['state'] = self.current_state
```

**FunciÃ³n `_get_level_from_state`** (lÃ­neas 170-178):

```python
def _get_level_from_state(self, state: str) -> ToxicityLevel:
    state_to_level = {
        'q0': ToxicityLevel.SAFE,
        'q1': ToxicityLevel.LOW,  # â† Retorna LOW
        'q2': ToxicityLevel.MEDIUM,
        'q3': ToxicityLevel.EXTREME
    }
    return state_to_level.get(state, ToxicityLevel.SAFE)
```

**Resultado**: `ToxicityLevel.LOW`

---

### Paso 7: Calcular Confianza (lÃ­neas 249-251)

```python
if detected_toxicity['is_toxic']:
    num_words = len(detected_toxicity['detected_words'])  # = 1
    detected_toxicity['confidence'] = min(1.0, num_words * 0.15)  # = 0.15
```

**CÃ¡lculo**: `min(1.0, 1 * 0.15) = 0.15` â†’ **15%**

---

### Paso 8: Generar Texto Resaltado (lÃ­neas 254-257)

```python
detected_toxicity['highlighted_text'] = self._highlight_detected_words(
    text, 
    detected_toxicity['detected_words']
)
```

**FunciÃ³n `_highlight_detected_words`** (lÃ­neas 261-325):
- Recibe: texto original y lista de palabras detectadas
- Genera HTML con `<span>` para resaltar palabras
- Retorna: `"Eres un <span style='background-color: #ffc107;...'>idiota</span>"`

---

## ğŸ“Š Resultado Final

```python
{
    'is_toxic': True,
    'level': ToxicityLevel.LOW,
    'state': 'q1',
    'types': ['insult'],
    'matched_patterns': [r'\bidiota\b'],
    'detected_words': [DetectedWord(text='idiota', start=9, end=15, ...)],
    'confidence': 0.15,
    'original_text': 'Eres un idiota',
    'highlighted_text': 'Eres un <span style="...">idiota</span>'
}
```

---

## ğŸ”„ Flujo Visual del CÃ³digo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ process_text("Eres un idiota")                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. reset() â†’ current_state = 'q0'                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Bucle: Buscar patrones en texto                         â”‚
â”‚    re.finditer(pattern, text)                               â”‚
â”‚    â†’ Encuentra "idiota" (INSULT)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Crear DetectedWord                                       â”‚
â”‚    - text: "idiota"                                         â”‚
â”‚    - start: 9, end: 15                                      â”‚
â”‚    - type: INSULT                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. _transition('q0', INSULT)                                â”‚
â”‚    â†’ Retorna 'q1'                                           â”‚
â”‚    â†’ current_state = 'q1'                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Actualizar informaciÃ³n                                   â”‚
â”‚    - is_toxic = True                                        â”‚
â”‚    - types.append('insult')                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. _get_level_from_state('q1')                              â”‚
â”‚    â†’ Retorna ToxicityLevel.LOW                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. Calcular confianza                                       â”‚
â”‚    confidence = min(1.0, 1 * 0.15) = 0.15                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. _highlight_detected_words()                              â”‚
â”‚    â†’ Genera HTML con palabras resaltadas                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 9. Retornar diccionario con resultados                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”‘ Funciones Clave del CÃ³digo

### 1. `_transition(current_state, toxicity_type)` (lÃ­neas 127-168)
**PropÃ³sito**: Aplica la funciÃ³n de transiciÃ³n Î´ del AFD

**LÃ³gica**:
- Si estado es `q3` â†’ siempre retorna `q3` (absorbente)
- Si estado es `q0`:
  - `INSULT` o `PROFANITY` â†’ `q1`
  - `HARASSMENT` â†’ `q2`
  - `THREAT` o `HATE` â†’ `q3`
- Si estado es `q1`:
  - `INSULT` o `PROFANITY` â†’ `q1` (permanece)
  - `HARASSMENT` â†’ `q2`
  - `THREAT` o `HATE` â†’ `q3`
- Si estado es `q2`:
  - `THREAT` o `HATE` â†’ `q3`
  - Otros â†’ `q2` (permanece)

### 2. `_get_level_from_state(state)` (lÃ­neas 170-178)
**PropÃ³sito**: Convierte estado del AFD (q0-q3) a nivel de toxicidad

**Mapeo**:
- `q0` â†’ `SAFE`
- `q1` â†’ `LOW`
- `q2` â†’ `MEDIUM`
- `q3` â†’ `EXTREME`

### 3. `_highlight_detected_words(text, detected_words)` (lÃ­neas 261-325)
**PropÃ³sito**: Genera HTML con palabras detectadas resaltadas

**Proceso**:
1. Ordena palabras por posiciÃ³n
2. Itera sobre el texto original
3. Inserta `<span>` con colores segÃºn tipo de toxicidad
4. Escapa HTML para prevenir XSS

---

## ğŸ’¡ Puntos Importantes

1. **El AFD procesa de izquierda a derecha**: Cada patrÃ³n encontrado aplica una transiciÃ³n inmediata.

2. **El estado se actualiza en cada detecciÃ³n**: `current_state` cambia cada vez que se encuentra un patrÃ³n.

3. **q3 es absorbente**: Una vez que el AFD llega a `q3`, nunca puede volver a estados anteriores.

4. **MÃºltiples patrones**: Si se detectan varios patrones, cada uno aplica su transiciÃ³n en orden.

5. **La confianza es acumulativa**: Se calcula al final basÃ¡ndose en el nÃºmero total de palabras detectadas.

---

## ğŸ“ Ejemplo con MÃºltiples Patrones

**Texto**: `"Eres un idiota, ojalÃ¡ te mueras"`

**Recorrido**:
1. Detecta `"idiota"` (INSULT) â†’ `q0` â†’ `q1` (LOW)
2. Detecta `"ojalÃ¡ te mueras"` (THREAT) â†’ `q1` â†’ `q3` (EXTREME)
3. **Estado final**: `q3` (EXTREME)
4. **Confianza**: `min(1.0, 2 * 0.15) = 0.30` (30%)

**CÃ³digo relevante**:
```python
# Primera detecciÃ³n
new_state = self._transition('q0', ToxicityType.INSULT)  # â†’ 'q1'
self.current_state = 'q1'

# Segunda detecciÃ³n
new_state = self._transition('q1', ToxicityType.THREAT)  # â†’ 'q3'
self.current_state = 'q3'  # Estado final
```

---

## ğŸ“ Ejemplo: Secuencia LOW â†’ MEDIUM â†’ EXTREME

**Texto**: "Eres un idiota, deja de acosarme, te voy a matar"

Este ejemplo ilustra cÃ³mo el autÃ³mata escala progresivamente cuando detecta tres patrones con severidad creciente.

### Paso 1: PatrÃ³n LOW (`INSULT`)

```python
new_state = self._transition('q0', ToxicityType.INSULT)  # â†’ 'q1'
self.current_state = 'q1'
```

- Estado anterior: `q0` (SAFE)
- PatrÃ³n detectado: "idiota" â†’ `INSULT`
- Nuevo estado: `q1` (LOW)

### Paso 2: PatrÃ³n MEDIUM (`HARASSMENT`)

```python
new_state = self._transition('q1', ToxicityType.HARASSMENT)  # â†’ 'q2'
self.current_state = 'q2'
```

- Estado anterior: `q1` (LOW)
- PatrÃ³n detectado: "deja de acosarme" â†’ `HARASSMENT`
- Nuevo estado: `q2` (MEDIUM)

### Paso 3: PatrÃ³n EXTREME (`THREAT`)

```python
new_state = self._transition('q2', ToxicityType.THREAT)  # â†’ 'q3'
self.current_state = 'q3'
```

- Estado anterior: `q2` (MEDIUM)
- PatrÃ³n detectado: "te voy a matar" â†’ `THREAT`
- Nuevo estado: `q3` (EXTREME)

### Resultado Final

- Estado final del AFD: `q3` (EXTREME)
- Tipos detectados: `['insult', 'harassment', 'threat']`
- Confianza: `min(1.0, 3 * 0.15) = 0.45` (45%)

